{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283f762e-d6d1-4005-a20c-06fba1f6df5f",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e7099b-62c5-44ec-9064-08fedcd33893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryanjadon/miniconda3/envs/papers_2025/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import pdb\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from importlib import resources\n",
    "from IPython.display import display, clear_output\n",
    "from chunking_evaluation.utils import rigorous_document_search\n",
    "from chunking_evaluation.evaluation_framework.base_evaluation import BaseEvaluation\n",
    "\n",
    "class SyntheticEvaluation(BaseEvaluation):\n",
    "    def __init__(self, corpora_paths: List[str], queries_csv_path: str, chroma_db_path: str = None,\n",
    "                 openai_api_key=None):\n",
    "        super().__init__(questions_csv_path=queries_csv_path, chroma_db_path=chroma_db_path)\n",
    "        self.corpora_paths = corpora_paths\n",
    "        self.questions_csv_path = queries_csv_path\n",
    "        self.client = OpenAI(base_url=\"Your-URL-HERE\", api_key=\"XXXX-Your-API-Key-Here-XXX\")\n",
    "        self.synth_questions_df = None\n",
    "\n",
    "        with resources.as_file(resources.files('chunking_evaluation.evaluation_framework') / 'prompts') as prompt_path:\n",
    "            with open(os.path.join(prompt_path, 'question_maker_system.txt'), 'r') as f:\n",
    "                self.question_maker_system_prompt = f.read()\n",
    "\n",
    "            with open(os.path.join(prompt_path, 'question_maker_approx_system.txt'), 'r') as f:\n",
    "                self.question_maker_approx_system_prompt = f.read()\n",
    "\n",
    "            with open(os.path.join(prompt_path, 'question_maker_user.txt'), 'r') as f:\n",
    "                self.question_maker_user_prompt = f.read()\n",
    "\n",
    "            with open(os.path.join(prompt_path, 'question_maker_approx_user.txt'), 'r') as f:\n",
    "                self.question_maker_approx_user_prompt = f.read()\n",
    "\n",
    "    def _save_questions_df(self):\n",
    "        self.synth_questions_df.to_csv(self.questions_csv_path, index=False)\n",
    "\n",
    "    def _tag_text(self, text):\n",
    "        chunk_length = 100\n",
    "        chunks = []\n",
    "        tag_indexes = [0]\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_length\n",
    "            chunk = text[start:end]\n",
    "            if end < len(text):\n",
    "                # Find the last space within the chunk to avoid splitting a word\n",
    "                space_index = chunk.rfind(' ')\n",
    "                if space_index != -1:\n",
    "                    end = start + space_index + 1  # Include the space in the chunk\n",
    "                    chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            tag_indexes.append(end)\n",
    "            start = end  # Move start to end to continue splitting\n",
    "\n",
    "        tagged_text = \"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            tagged_text += f\"<start_chunk_{i}>\" + chunk + f\"<end_chunk_{i}>\"\n",
    "\n",
    "        return tagged_text, tag_indexes\n",
    "\n",
    "    def convert_text_to_json(self,text: str):\n",
    "        \"\"\"\n",
    "        Removes the text between <think> and </think> tags and converts\n",
    "        the remaining text (assumed to be a Python dictionary literal)\n",
    "        into a JSON string.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The original text containing a <think>...</think> block\n",
    "                        and the dictionary to convert.\n",
    "    \n",
    "        Returns:\n",
    "            str: A JSON-formatted string representing the dictionary.\n",
    "        \"\"\"\n",
    "        # Remove everything between <think> and </think> (including the tags)\n",
    "        cleaned_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "        # Strip any leading/trailing whitespace\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "    \n",
    "        # Parse the remaining text as a Python literal.\n",
    "        # (Using ast.literal_eval since the text is written as a Python dictionary.)\n",
    "        try:\n",
    "            data = ast.literal_eval(cleaned_text)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Failed to parse the remaining text as a dictionary: \" + str(e))\n",
    "    \n",
    "        # Convert the Python dictionary to a JSON-formatted string\n",
    "        json_response = json.dumps(data, indent=2)\n",
    "        \n",
    "        return json_response\n",
    "\n",
    "    def _extract_question_and_approx_references(self, corpus, document_length=4000, prev_questions=[]):\n",
    "        \n",
    "        if len(corpus) > document_length:\n",
    "            start_index = random.randint(0, len(corpus) - document_length)\n",
    "            document = corpus[start_index: start_index + document_length]\n",
    "        else:\n",
    "            start_index = 0\n",
    "            document = corpus\n",
    "\n",
    "        if prev_questions is not None:\n",
    "            if len(prev_questions) > 20:\n",
    "                questions_sample = random.sample(prev_questions, 20)\n",
    "                prev_questions_str = '\\n'.join(questions_sample)\n",
    "            else:\n",
    "                prev_questions_str = '\\n'.join(prev_questions)\n",
    "        else:\n",
    "            prev_questions_str = \"\"\n",
    "\n",
    "        tagged_text, tag_indexes = self._tag_text(document)\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.question_maker_approx_system_prompt},\n",
    "                {\"role\": \"user\",\n",
    "                 \"content\": self.question_maker_approx_user_prompt.replace(\"{document}\", tagged_text).replace(\n",
    "                     \"{prev_questions_str}\", prev_questions_str)}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response = completion.choices[0].message.content\n",
    "        json_response = json.loads(self.convert_text_to_json(response))\n",
    "        \n",
    "        print(json_response)\n",
    "        \n",
    "        try:\n",
    "            text_references = json_response['references']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"The response does not contain a 'references' field.\")\n",
    "\n",
    "        try:\n",
    "            question = json_response['question']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"The response does not contain a 'question' field.\")\n",
    "\n",
    "        \n",
    "        references = []\n",
    "    \n",
    "        for reference in text_references:\n",
    "            try:\n",
    "                reference_keys = list(reference.keys())\n",
    "    \n",
    "                if len(reference_keys) != 3:\n",
    "                    raise ValueError(\n",
    "                        f\"Each reference must have exactly 3 keys: 'content', 'start_chunk', and 'end_chunk'. Got keys: {reference_keys}\")\n",
    "    \n",
    "                if 'start_chunk' not in reference_keys or 'end_chunk' not in reference_keys:\n",
    "                    raise ValueError(\"Each reference must contain 'start_chunk' and 'end_chunk' keys.\")\n",
    "    \n",
    "                if 'end_chunk' not in reference_keys:\n",
    "                    reference_keys.remove('content')\n",
    "                    reference_keys.remove('start_chunk')\n",
    "                    end_chunk_key = reference_keys[0]\n",
    "                    end_index = start_index + tag_indexes[reference[end_chunk_key] + 1]\n",
    "                else:\n",
    "                    end_index = start_index + tag_indexes[reference['end_chunk'] + 1]\n",
    "    \n",
    "                start_index = start_index + tag_indexes[reference['start_chunk']]\n",
    "                references.append((corpus[start_index:end_index], start_index, end_index))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "        return question, references\n",
    "\n",
    "    def _extract_question_and_references(self, corpus, document_length=4000, prev_questions=[]):\n",
    "        if len(corpus) > document_length:\n",
    "            start_index = random.randint(0, len(corpus) - document_length)\n",
    "            document = corpus[start_index: start_index + document_length]\n",
    "        else:\n",
    "            document = corpus\n",
    "\n",
    "        if prev_questions is not None:\n",
    "            if len(prev_questions) > 20:\n",
    "                questions_sample = random.sample(prev_questions, 20)\n",
    "                prev_questions_str = '\\n'.join(questions_sample)\n",
    "            else:\n",
    "                prev_questions_str = '\\n'.join(prev_questions)\n",
    "        else:\n",
    "            prev_questions_str = \"\"\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.question_maker_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": self.question_maker_user_prompt.replace(\"{document}\", document).replace(\n",
    "                    \"{prev_questions_str}\", prev_questions_str)}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response = completion.choices[0].message\n",
    "        json_response = json.loads(response.json())\n",
    "        \n",
    "        input_str = json_response['content']\n",
    "        new_string = input_str.replace(\"```json\", \"```\")\n",
    "        cleaned_string = new_string.strip().strip('```').strip()\n",
    "            \n",
    "        parsed_dict = json.loads(input_str)\n",
    "        print(parsed_dict)\n",
    "        \n",
    "        json_response = parsed_dict\n",
    "\n",
    "\n",
    "        try:\n",
    "            text_references = json_response['references']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"The response does not contain a 'references' field.\")\n",
    "\n",
    "        try:\n",
    "            question = json_response['question']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"The response does not contain a 'question' field.\")\n",
    "\n",
    "        references = []\n",
    "        for reference in text_references:\n",
    "            if not isinstance(reference, str):\n",
    "                raise ValueError(f\"Expected reference to be of type str, but got {type(reference).__name__}\")\n",
    "            target = rigorous_document_search(corpus, reference)\n",
    "            if target is not None:\n",
    "                reference, start_index, end_index = target\n",
    "                references.append((reference, start_index, end_index))\n",
    "            else:\n",
    "                raise ValueError(f\"No match found in the document for the given reference.\\nReference: {reference}\")\n",
    "\n",
    "        return question, references\n",
    "\n",
    "    def _generate_corpus_questions(self, corpus_id, approx=False, n=5):\n",
    "        with open(corpus_id, 'r') as file:\n",
    "            corpus = file.read()\n",
    "\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            while True:\n",
    "                try:\n",
    "                    print(f\"Trying Query {i}\")\n",
    "                    questions_list = self.synth_questions_df[self.synth_questions_df['corpus_id'] == corpus_id][\n",
    "                        'question'].tolist()\n",
    "                    if approx:\n",
    "                        question, references = self._extract_question_and_approx_references(corpus, 4000,\n",
    "                                                                                            questions_list)\n",
    "                    else:\n",
    "                        question, references = self._extract_question_and_references(corpus, 4000, questions_list)\n",
    "                    if len(references) > 5:\n",
    "                        raise ValueError(\"The number of references exceeds 5.\")\n",
    "\n",
    "                    references = [{'content': ref[0], 'start_index': ref[1], 'end_index': ref[2]} for ref in references]\n",
    "                    new_question = {\n",
    "                        'question': question,\n",
    "                        'references': json.dumps(references),\n",
    "                        'corpus_id': corpus_id\n",
    "                    }\n",
    "\n",
    "                    new_df = pd.DataFrame([new_question])\n",
    "                    self.synth_questions_df = pd.concat([self.synth_questions_df, new_df], ignore_index=True)\n",
    "                    self._save_questions_df()\n",
    "\n",
    "                    break\n",
    "                except (ValueError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error occurred: {e}\")\n",
    "                    continue\n",
    "            i += 1\n",
    "\n",
    "    def _get_synth_questions_df(self):\n",
    "        if os.path.exists(self.questions_csv_path):\n",
    "            synth_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "        else:\n",
    "            synth_questions_df = pd.DataFrame(columns=['question', 'references', 'corpus_id'])\n",
    "        return synth_questions_df\n",
    "\n",
    "    def generate_queries_and_excerpts(self, approximate_excerpts=False, num_rounds=-1, queries_per_corpus=5):\n",
    "        self.synth_questions_df = self._get_synth_questions_df()\n",
    "\n",
    "        rounds = 0\n",
    "        while num_rounds == -1 or rounds < num_rounds:\n",
    "            for corpus_id in self.corpora_paths:\n",
    "                self._generate_corpus_questions(corpus_id, approx=approximate_excerpts, n=queries_per_corpus)\n",
    "            rounds += 1\n",
    "\n",
    "    def _get_sim(self, target, references):\n",
    "        response = self.client.embeddings.create(\n",
    "            input=[target] + references,\n",
    "            model=\"text-embedding-bge-m3\"\n",
    "        )\n",
    "        nparray1 = np.array(response.data[0].embedding)\n",
    "\n",
    "        full_sim = []\n",
    "        for i in range(1, len(response.data)):\n",
    "            nparray2 = np.array(response.data[i].embedding)\n",
    "            cosine_similarity = np.dot(nparray1, nparray2) / (np.linalg.norm(nparray1) * np.linalg.norm(nparray2))\n",
    "            full_sim.append(cosine_similarity)\n",
    "\n",
    "        return full_sim\n",
    "\n",
    "    def _corpus_filter_poor_highlights(self, corpus_id, synth_questions_df, threshold):\n",
    "        corpus_questions_df = synth_questions_df[synth_questions_df['corpus_id'] == corpus_id]\n",
    "\n",
    "        def edit_row(row):\n",
    "            question = row['question']\n",
    "            references = [ref['content'] for ref in row['references']]\n",
    "            similarity_scores = self._get_sim(question, references)\n",
    "            worst_ref_score = min(similarity_scores)\n",
    "            row['worst_ref_score'] = worst_ref_score\n",
    "            return row\n",
    "\n",
    "        # Apply the function to each row\n",
    "        corpus_questions_df = corpus_questions_df.apply(edit_row, axis=1)\n",
    "\n",
    "        count_before = len(corpus_questions_df)\n",
    "\n",
    "        corpus_questions_df = corpus_questions_df[corpus_questions_df['worst_ref_score'] >= threshold]\n",
    "        corpus_questions_df = corpus_questions_df.drop(columns=['worst_ref_score'])\n",
    "\n",
    "        count_after = len(corpus_questions_df)\n",
    "\n",
    "        print(f\"Corpus: {corpus_id} - Removed {count_before - count_after} .\")\n",
    "\n",
    "        corpus_questions_df['references'] = corpus_questions_df['references'].apply(json.dumps)\n",
    "\n",
    "        full_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "        full_questions_df = full_questions_df[full_questions_df['corpus_id'] != corpus_id]\n",
    "\n",
    "        full_questions_df = pd.concat([full_questions_df, corpus_questions_df], ignore_index=True)\n",
    "        # Drop the columns 'fixed', 'worst_ref_score' and 'diff_score' if they exist\n",
    "        for col in ['fixed', 'worst_ref_score', 'diff_score']:\n",
    "            if col in full_questions_df.columns:\n",
    "                full_questions_df = full_questions_df.drop(columns=col)\n",
    "\n",
    "        full_questions_df.to_csv(self.questions_csv_path, index=False)\n",
    "\n",
    "    def filter_poor_excerpts(self, threshold=0.36, corpora_subset=[]):\n",
    "        if os.path.exists(self.questions_csv_path):\n",
    "            synth_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "            if len(synth_questions_df) > 0:\n",
    "                synth_questions_df['references'] = synth_questions_df['references'].apply(json.loads)\n",
    "                corpus_list = synth_questions_df['corpus_id'].unique().tolist()\n",
    "                if corpora_subset:\n",
    "                    corpus_list = [c for c in corpus_list if c in corpora_subset]\n",
    "                for corpus_id in corpus_list:\n",
    "                    self._corpus_filter_poor_highlights(corpus_id, synth_questions_df, threshold)\n",
    "\n",
    "    def _corpus_filter_duplicates(self, corpus_id, synth_questions_df, threshold):\n",
    "        corpus_questions_df = synth_questions_df[synth_questions_df['corpus_id'] == corpus_id].copy()\n",
    "\n",
    "        count_before = len(corpus_questions_df)\n",
    "\n",
    "        corpus_questions_df.drop_duplicates(subset='question', keep='first', inplace=True)\n",
    "\n",
    "        questions = corpus_questions_df['question'].tolist()\n",
    "\n",
    "        response = self.client.embeddings.create(\n",
    "            input=questions,\n",
    "            model=\"text-embedding-bge-m3\"\n",
    "        )\n",
    "\n",
    "        embeddings_matrix = np.array([data.embedding for data in response.data])\n",
    "\n",
    "        dot_product_matrix = np.dot(embeddings_matrix, embeddings_matrix.T)\n",
    "\n",
    "        # Create a list of tuples containing the index pairs and their similarity\n",
    "        similarity_pairs = [(i, j, dot_product_matrix[i][j]) for i in range(len(dot_product_matrix)) for j in\n",
    "                            range(i + 1, len(dot_product_matrix))]\n",
    "\n",
    "        # Sort the list of tuples based on the similarity in descending order\n",
    "        similarity_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        similarity_scores = np.array([x[2] for x in similarity_pairs])\n",
    "\n",
    "        most_similars = (dot_product_matrix - np.eye(dot_product_matrix.shape[0])).max(axis=1)\n",
    "\n",
    "        def filter_vectors(sim_matrix, threshold):\n",
    "            n = sim_matrix.shape[0]  # Number of vectors\n",
    "            remaining = np.ones(n, dtype=bool)  # Initialize all vectors as remaining\n",
    "\n",
    "            for i in range(n):\n",
    "                if remaining[i] == 1:  # Only check for vectors that are still remaining\n",
    "                    for j in range(i + 1, n):\n",
    "                        if remaining[j] == 1 and sim_matrix[i, j] > threshold:\n",
    "                            remaining[j] = 0  # Remove vector j because it's too similar to vector i\n",
    "\n",
    "            return remaining\n",
    "\n",
    "        rows_to_keep = filter_vectors(dot_product_matrix, threshold)\n",
    "\n",
    "        corpus_questions_df = corpus_questions_df[rows_to_keep]\n",
    "\n",
    "        count_after = len(corpus_questions_df)\n",
    "\n",
    "        print(f\"Corpus: {corpus_id} - Removed {count_before - count_after} .\")\n",
    "\n",
    "        corpus_questions_df['references'] = corpus_questions_df['references'].apply(json.dumps)\n",
    "\n",
    "        full_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "        full_questions_df = full_questions_df[full_questions_df['corpus_id'] != corpus_id]\n",
    "\n",
    "        full_questions_df = pd.concat([full_questions_df, corpus_questions_df], ignore_index=True)\n",
    "        # Drop the columns 'fixed', 'worst_ref_score' and 'diff_score' if they exist\n",
    "        for col in ['fixed', 'worst_ref_score', 'diff_score']:\n",
    "            if col in full_questions_df.columns:\n",
    "                full_questions_df = full_questions_df.drop(columns=col)\n",
    "\n",
    "        full_questions_df.to_csv(self.questions_csv_path, index=False)\n",
    "\n",
    "    def filter_duplicates(self, threshold=0.78, corpora_subset=[]):\n",
    "        if os.path.exists(self.questions_csv_path):\n",
    "            synth_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "            if len(synth_questions_df) > 0:\n",
    "                synth_questions_df['references'] = synth_questions_df['references'].apply(json.loads)\n",
    "                corpus_list = synth_questions_df['corpus_id'].unique().tolist()\n",
    "                if corpora_subset:\n",
    "                    corpus_list = [c for c in corpus_list if c in corpora_subset]\n",
    "                for corpus_id in corpus_list:\n",
    "                    self._corpus_filter_duplicates(corpus_id, synth_questions_df, threshold)\n",
    "\n",
    "    def question_ref_filter(self):\n",
    "        self.synth_questions_df = self._get_synth_questions_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1e115-8585-445a-ad3c-43b0ea8eb577",
   "metadata": {},
   "source": [
    "### Add Generated QA Pairs from Reasoning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45f6f6d-83eb-4795-b344-19698de0423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_paths = []\n",
    "csv_path = 'Final-Outputs/pubmed_results_phi-4.csv'\n",
    "synthetic_pipeline = SyntheticEvaluation(corpora_paths, csv_path)\n",
    "synthetic_df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4c66a8-279f-47a1-b931-d4fcda202cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>references</th>\n",
       "      <th>corpus_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What specific RSS shows no obvious explanation...</td>\n",
       "      <td>[{\"content\": \"study of individual RSS nucleoti...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which hypothetical protein is graphically show...</td>\n",
       "      <td>[{\"content\": \"annotation. The internal consist...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the advantages of using Xenopus egg e...</td>\n",
       "      <td>[{\"content\": \"model. There are several unusual...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the result of excluding A+T-rich codon...</td>\n",
       "      <td>[{\"content\": \"methione, isoleucine, asparagine...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the updated date for the content descr...</td>\n",
       "      <td>[{\"content\": \"step Mol Cell Biol 1998 18 6408 ...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the two endosymbionts mentioned in th...</td>\n",
       "      <td>[{\"content\": \"reconstructing the events of gen...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which experiment demonstrated that brefeldin A...</td>\n",
       "      <td>[{\"content\": \"cDNA encoding a bovine brain bre...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who manages Creative Commons?</td>\n",
       "      <td>[{\"content\": \" copyright requires, oddly, some...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Which ARNO mutants did not co-immunoprecipitat...</td>\n",
       "      <td>[{\"content\": \"the insulin receptor upon insuli...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>At what time does the peak expression of ribon...</td>\n",
       "      <td>[{\"content\": \"ribonucleotide synthesis is requ...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How do consensus spacer sequences affect the f...</td>\n",
       "      <td>[{\"content\": \"of a consensus spacer may allow ...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Which RSS associated with the murine Jβ2.6 pse...</td>\n",
       "      <td>[{\"content\": \"elements and positions in the RS...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What are the three components that make up eac...</td>\n",
       "      <td>[{\"content\": \"presence is both necessary and s...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Which protein is secreted by ventral lateral n...</td>\n",
       "      <td>[{\"content\": \"rhythms depend on other neuronal...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What criteria were used to retain quality spot...</td>\n",
       "      <td>[{\"content\": \"spots were retained based on the...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the genetic distance between the Borne...</td>\n",
       "      <td>[{\"content\": \"network of haplotypes (Figure 4)...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Which organizations supported RH in this study?</td>\n",
       "      <td>[{\"content\": \"itute of General Medical Science...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What are the factors driving the division betw...</td>\n",
       "      <td>[{\"content\": \"meaningful,\\u201d says Persley.\\...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What are the determinants of RSS specificity a...</td>\n",
       "      <td>[{\"content\": \"themselves demonstrate a remarka...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the observed phase change in Pdf01 mut...</td>\n",
       "      <td>[{\"content\": \"advanced phase were observed in ...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does APC-mediated degradation of axin affe...</td>\n",
       "      <td>[{\"content\": \"APC-mediated degradation of axin...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What methods did the researchers use to determ...</td>\n",
       "      <td>[{\"content\": \"debate.\\n\\nApplying DNA analysis...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Who conducted the DNA analysis to determine th...</td>\n",
       "      <td>[{\"content\": \"debate.\\n\\nApplying DNA analysis...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How many loci required for viability were dete...</td>\n",
       "      <td>[{\"content\": \"phenotype described in the liter...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Which study explores the role of SERA genes in...</td>\n",
       "      <td>[{\"content\": \"31206\\nMiller SK  Good RT  Drew ...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What is the role of Creative Commons licenses ...</td>\n",
       "      <td>[{\"content\": \"sciences.\\n\\nThe Massachusetts I...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What specific conditions were used to simulate...</td>\n",
       "      <td>[{\"content\": \"in the legends of Table 1 and 2....</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What stage of the P. falciparum life cycle is ...</td>\n",
       "      <td>[{\"content\": \"intraerythrocytic developmental ...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What was the primary function of the rrf-3 mut...</td>\n",
       "      <td>[{\"content\": \"Feeding of this RNAi library to ...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Who are the authors of 'Identification and uti...</td>\n",
       "      <td>[{\"content\": \"7594539\\nCowell LG  Davila M  Ke...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>At what specific points during a circadian day...</td>\n",
       "      <td>[{\"content\": \"LD.\\n\\n(C) Brains taken in the f...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>How many genes are induced during the ring to ...</td>\n",
       "      <td>[{\"content\": \"expression phases based on their...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Where in the brain was the strong PDF peptide ...</td>\n",
       "      <td>[{\"content\": \"as their projections (right pane...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>What percentage of the Plasmodium falciparum g...</td>\n",
       "      <td>[{\"content\": \"The Plasmodium parasite devotes ...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>How do researchers examine discordance in topo...</td>\n",
       "      <td>[{\"content\": \"topology and two gene alignments...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>What strongly suggests that the Borneo elephan...</td>\n",
       "      <td>[{\"content\": \"of elephants (Fernando and Lande...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>How did the presence of insulin influence ARF1...</td>\n",
       "      <td>[{\"content\": \"ized on the surface of the cells...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>What is the ratio of dissociation constants fo...</td>\n",
       "      <td>[{\"content\": \"diffusion limits for a typical p...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>What effect does increasing the concentration ...</td>\n",
       "      <td>[{\"content\": \"concentration of either axin or ...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>What is the half-life of β-catenin when nonaxi...</td>\n",
       "      <td>[{\"content\": \"under certain conditions. In our...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>What is the range of new merozoites formed thr...</td>\n",
       "      <td>[{\"content\": \"y replicating and dividing to fo...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>What percentage of the Plasmodium genome is de...</td>\n",
       "      <td>[{\"content\": \"determining the status of the li...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>What is the license type and citation for the ...</td>\n",
       "      <td>[{\"content\": \"P\\nDrosophila  clock can generat...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Which proteases are implicated in the merozoit...</td>\n",
       "      <td>[{\"content\": \"mid-trophozoite stage, during pe...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How does the substitution of individual consen...</td>\n",
       "      <td>[{\"content\": \"those obtained in the other assa...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>What criteria were used to eliminate nonhomolo...</td>\n",
       "      <td>[{\"content\": \"Sequences\\n\\na\\u2009Chromosome 1...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Which gene was identified in the cloning of ge...</td>\n",
       "      <td>[{\"content\": \"exon\\n\\nThe identification of mu...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Which recent study involving Drosophila sugges...</td>\n",
       "      <td>[{\"content\": \"influences clock cells on the op...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>What role does insulin play in the translocati...</td>\n",
       "      <td>[{\"content\": \"[34].\\n\\nOur studies showed that...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Which publications relate to the sequence evol...</td>\n",
       "      <td>[{\"content\": \"sequence data sets Nat Genet 200...</td>\n",
       "      <td>chunking_evaluation/pubmed/pubmed.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What specific RSS shows no obvious explanation...   \n",
       "1   Which hypothetical protein is graphically show...   \n",
       "2   What are the advantages of using Xenopus egg e...   \n",
       "3   What is the result of excluding A+T-rich codon...   \n",
       "4   What is the updated date for the content descr...   \n",
       "5   What are the two endosymbionts mentioned in th...   \n",
       "6   Which experiment demonstrated that brefeldin A...   \n",
       "7                       Who manages Creative Commons?   \n",
       "8   Which ARNO mutants did not co-immunoprecipitat...   \n",
       "9   At what time does the peak expression of ribon...   \n",
       "10  How do consensus spacer sequences affect the f...   \n",
       "11  Which RSS associated with the murine Jβ2.6 pse...   \n",
       "12  What are the three components that make up eac...   \n",
       "13  Which protein is secreted by ventral lateral n...   \n",
       "14  What criteria were used to retain quality spot...   \n",
       "15  What is the genetic distance between the Borne...   \n",
       "16    Which organizations supported RH in this study?   \n",
       "17  What are the factors driving the division betw...   \n",
       "18  What are the determinants of RSS specificity a...   \n",
       "19  What is the observed phase change in Pdf01 mut...   \n",
       "20  How does APC-mediated degradation of axin affe...   \n",
       "21  What methods did the researchers use to determ...   \n",
       "22  Who conducted the DNA analysis to determine th...   \n",
       "23  How many loci required for viability were dete...   \n",
       "24  Which study explores the role of SERA genes in...   \n",
       "25  What is the role of Creative Commons licenses ...   \n",
       "26  What specific conditions were used to simulate...   \n",
       "27  What stage of the P. falciparum life cycle is ...   \n",
       "28  What was the primary function of the rrf-3 mut...   \n",
       "29  Who are the authors of 'Identification and uti...   \n",
       "30  At what specific points during a circadian day...   \n",
       "31  How many genes are induced during the ring to ...   \n",
       "32  Where in the brain was the strong PDF peptide ...   \n",
       "33  What percentage of the Plasmodium falciparum g...   \n",
       "34  How do researchers examine discordance in topo...   \n",
       "35  What strongly suggests that the Borneo elephan...   \n",
       "36  How did the presence of insulin influence ARF1...   \n",
       "37  What is the ratio of dissociation constants fo...   \n",
       "38  What effect does increasing the concentration ...   \n",
       "39  What is the half-life of β-catenin when nonaxi...   \n",
       "40  What is the range of new merozoites formed thr...   \n",
       "41  What percentage of the Plasmodium genome is de...   \n",
       "42  What is the license type and citation for the ...   \n",
       "43  Which proteases are implicated in the merozoit...   \n",
       "44  How does the substitution of individual consen...   \n",
       "45  What criteria were used to eliminate nonhomolo...   \n",
       "46  Which gene was identified in the cloning of ge...   \n",
       "47  Which recent study involving Drosophila sugges...   \n",
       "48  What role does insulin play in the translocati...   \n",
       "49  Which publications relate to the sequence evol...   \n",
       "\n",
       "                                           references  \\\n",
       "0   [{\"content\": \"study of individual RSS nucleoti...   \n",
       "1   [{\"content\": \"annotation. The internal consist...   \n",
       "2   [{\"content\": \"model. There are several unusual...   \n",
       "3   [{\"content\": \"methione, isoleucine, asparagine...   \n",
       "4   [{\"content\": \"step Mol Cell Biol 1998 18 6408 ...   \n",
       "5   [{\"content\": \"reconstructing the events of gen...   \n",
       "6   [{\"content\": \"cDNA encoding a bovine brain bre...   \n",
       "7   [{\"content\": \" copyright requires, oddly, some...   \n",
       "8   [{\"content\": \"the insulin receptor upon insuli...   \n",
       "9   [{\"content\": \"ribonucleotide synthesis is requ...   \n",
       "10  [{\"content\": \"of a consensus spacer may allow ...   \n",
       "11  [{\"content\": \"elements and positions in the RS...   \n",
       "12  [{\"content\": \"presence is both necessary and s...   \n",
       "13  [{\"content\": \"rhythms depend on other neuronal...   \n",
       "14  [{\"content\": \"spots were retained based on the...   \n",
       "15  [{\"content\": \"network of haplotypes (Figure 4)...   \n",
       "16  [{\"content\": \"itute of General Medical Science...   \n",
       "17  [{\"content\": \"meaningful,\\u201d says Persley.\\...   \n",
       "18  [{\"content\": \"themselves demonstrate a remarka...   \n",
       "19  [{\"content\": \"advanced phase were observed in ...   \n",
       "20  [{\"content\": \"APC-mediated degradation of axin...   \n",
       "21  [{\"content\": \"debate.\\n\\nApplying DNA analysis...   \n",
       "22  [{\"content\": \"debate.\\n\\nApplying DNA analysis...   \n",
       "23  [{\"content\": \"phenotype described in the liter...   \n",
       "24  [{\"content\": \"31206\\nMiller SK  Good RT  Drew ...   \n",
       "25  [{\"content\": \"sciences.\\n\\nThe Massachusetts I...   \n",
       "26  [{\"content\": \"in the legends of Table 1 and 2....   \n",
       "27  [{\"content\": \"intraerythrocytic developmental ...   \n",
       "28  [{\"content\": \"Feeding of this RNAi library to ...   \n",
       "29  [{\"content\": \"7594539\\nCowell LG  Davila M  Ke...   \n",
       "30  [{\"content\": \"LD.\\n\\n(C) Brains taken in the f...   \n",
       "31  [{\"content\": \"expression phases based on their...   \n",
       "32  [{\"content\": \"as their projections (right pane...   \n",
       "33  [{\"content\": \"The Plasmodium parasite devotes ...   \n",
       "34  [{\"content\": \"topology and two gene alignments...   \n",
       "35  [{\"content\": \"of elephants (Fernando and Lande...   \n",
       "36  [{\"content\": \"ized on the surface of the cells...   \n",
       "37  [{\"content\": \"diffusion limits for a typical p...   \n",
       "38  [{\"content\": \"concentration of either axin or ...   \n",
       "39  [{\"content\": \"under certain conditions. In our...   \n",
       "40  [{\"content\": \"y replicating and dividing to fo...   \n",
       "41  [{\"content\": \"determining the status of the li...   \n",
       "42  [{\"content\": \"P\\nDrosophila  clock can generat...   \n",
       "43  [{\"content\": \"mid-trophozoite stage, during pe...   \n",
       "44  [{\"content\": \"those obtained in the other assa...   \n",
       "45  [{\"content\": \"Sequences\\n\\na\\u2009Chromosome 1...   \n",
       "46  [{\"content\": \"exon\\n\\nThe identification of mu...   \n",
       "47  [{\"content\": \"influences clock cells on the op...   \n",
       "48  [{\"content\": \"[34].\\n\\nOur studies showed that...   \n",
       "49  [{\"content\": \"sequence data sets Nat Genet 200...   \n",
       "\n",
       "                                corpus_id  \n",
       "0   chunking_evaluation/pubmed/pubmed.txt  \n",
       "1   chunking_evaluation/pubmed/pubmed.txt  \n",
       "2   chunking_evaluation/pubmed/pubmed.txt  \n",
       "3   chunking_evaluation/pubmed/pubmed.txt  \n",
       "4   chunking_evaluation/pubmed/pubmed.txt  \n",
       "5   chunking_evaluation/pubmed/pubmed.txt  \n",
       "6   chunking_evaluation/pubmed/pubmed.txt  \n",
       "7   chunking_evaluation/pubmed/pubmed.txt  \n",
       "8   chunking_evaluation/pubmed/pubmed.txt  \n",
       "9   chunking_evaluation/pubmed/pubmed.txt  \n",
       "10  chunking_evaluation/pubmed/pubmed.txt  \n",
       "11  chunking_evaluation/pubmed/pubmed.txt  \n",
       "12  chunking_evaluation/pubmed/pubmed.txt  \n",
       "13  chunking_evaluation/pubmed/pubmed.txt  \n",
       "14  chunking_evaluation/pubmed/pubmed.txt  \n",
       "15  chunking_evaluation/pubmed/pubmed.txt  \n",
       "16  chunking_evaluation/pubmed/pubmed.txt  \n",
       "17  chunking_evaluation/pubmed/pubmed.txt  \n",
       "18  chunking_evaluation/pubmed/pubmed.txt  \n",
       "19  chunking_evaluation/pubmed/pubmed.txt  \n",
       "20  chunking_evaluation/pubmed/pubmed.txt  \n",
       "21  chunking_evaluation/pubmed/pubmed.txt  \n",
       "22  chunking_evaluation/pubmed/pubmed.txt  \n",
       "23  chunking_evaluation/pubmed/pubmed.txt  \n",
       "24  chunking_evaluation/pubmed/pubmed.txt  \n",
       "25  chunking_evaluation/pubmed/pubmed.txt  \n",
       "26  chunking_evaluation/pubmed/pubmed.txt  \n",
       "27  chunking_evaluation/pubmed/pubmed.txt  \n",
       "28  chunking_evaluation/pubmed/pubmed.txt  \n",
       "29  chunking_evaluation/pubmed/pubmed.txt  \n",
       "30  chunking_evaluation/pubmed/pubmed.txt  \n",
       "31  chunking_evaluation/pubmed/pubmed.txt  \n",
       "32  chunking_evaluation/pubmed/pubmed.txt  \n",
       "33  chunking_evaluation/pubmed/pubmed.txt  \n",
       "34  chunking_evaluation/pubmed/pubmed.txt  \n",
       "35  chunking_evaluation/pubmed/pubmed.txt  \n",
       "36  chunking_evaluation/pubmed/pubmed.txt  \n",
       "37  chunking_evaluation/pubmed/pubmed.txt  \n",
       "38  chunking_evaluation/pubmed/pubmed.txt  \n",
       "39  chunking_evaluation/pubmed/pubmed.txt  \n",
       "40  chunking_evaluation/pubmed/pubmed.txt  \n",
       "41  chunking_evaluation/pubmed/pubmed.txt  \n",
       "42  chunking_evaluation/pubmed/pubmed.txt  \n",
       "43  chunking_evaluation/pubmed/pubmed.txt  \n",
       "44  chunking_evaluation/pubmed/pubmed.txt  \n",
       "45  chunking_evaluation/pubmed/pubmed.txt  \n",
       "46  chunking_evaluation/pubmed/pubmed.txt  \n",
       "47  chunking_evaluation/pubmed/pubmed.txt  \n",
       "48  chunking_evaluation/pubmed/pubmed.txt  \n",
       "49  chunking_evaluation/pubmed/pubmed.txt  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13613f7e-8332-42df-a8f2-705190042494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "from chunking_evaluation import GeneralEvaluation, BaseChunker\n",
    "\n",
    "class SentenceChunker(BaseChunker):\n",
    "    def __init__(self, sentences_per_chunk: int = 3):\n",
    "        # Initialize the chunker with the number of sentences per chunk\n",
    "        self.sentences_per_chunk = sentences_per_chunk\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        # Handle the case where the input text is empty\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # Split the input text into sentences using regular expression\n",
    "        # Regex looks for white space following . ! or ? and makes a split\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        chunks = []\n",
    "\n",
    "        # Group sentences into chunks based on the specified number\n",
    "        for i in range(0, len(sentences), self.sentences_per_chunk):\n",
    "            # Combine sentences into a single chunk\n",
    "            chunk = ' '.join(sentences[i:i + self.sentences_per_chunk])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Return the list of chunks\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6a58d1-7bc7-43f6-afd0-bb3683535e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def print_metrics(results):\n",
    "    \n",
    "    # Grab Summary Metrics    \n",
    "    metrics = {\n",
    "        'Recall': (results['recall_mean'], results['recall_std']),\n",
    "        'Precision': (results['precision_mean'], results['precision_std']),\n",
    "        'Precision Ω': (results['precision_omega_mean'], results['precision_omega_std']),\n",
    "        'IoU': (results['iou_mean'], results['iou_std'])\n",
    "    }\n",
    "    \n",
    "    # Print each metric with mean ± std\n",
    "    for metric, (mean, std) in metrics.items():\n",
    "        print(f\"{metric}: {mean:.4f} ± {std:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2e079b-347e-459c-aa8f-d8cfb043c421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iou_mean</th>\n",
       "      <th>iou_std</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>precision_omega_mean</th>\n",
       "      <th>precision_omega_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>chunker</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>embedding_function</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.071656</td>\n",
       "      <td>0.050754</td>\n",
       "      <td>0.700689</td>\n",
       "      <td>0.379748</td>\n",
       "      <td>0.281755</td>\n",
       "      <td>0.147734</td>\n",
       "      <td>0.073431</td>\n",
       "      <td>0.050889</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>5</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_5_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.042006</td>\n",
       "      <td>0.029751</td>\n",
       "      <td>0.722089</td>\n",
       "      <td>0.391521</td>\n",
       "      <td>0.194373</td>\n",
       "      <td>0.114295</td>\n",
       "      <td>0.042541</td>\n",
       "      <td>0.029839</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>10</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_10_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.739775</td>\n",
       "      <td>0.378726</td>\n",
       "      <td>0.122446</td>\n",
       "      <td>0.065091</td>\n",
       "      <td>0.028013</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>15</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_15_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.014792</td>\n",
       "      <td>0.722592</td>\n",
       "      <td>0.384038</td>\n",
       "      <td>0.104408</td>\n",
       "      <td>0.059636</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.014868</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>20</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_20_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iou_mean   iou_std  recall_mean  recall_std  precision_omega_mean  \\\n",
       "0  0.071656  0.050754     0.700689    0.379748              0.281755   \n",
       "1  0.042006  0.029751     0.722089    0.391521              0.194373   \n",
       "2  0.027767  0.020774     0.739775    0.378726              0.122446   \n",
       "3  0.020538  0.014792     0.722592    0.384038              0.104408   \n",
       "\n",
       "   precision_omega_std  precision_mean  precision_std          chunker  \\\n",
       "0             0.147734        0.073431       0.050889  SentenceChunker   \n",
       "1             0.114295        0.042541       0.029839  SentenceChunker   \n",
       "2             0.065091        0.028013       0.020985  SentenceChunker   \n",
       "3             0.059636        0.020700       0.014868  SentenceChunker   \n",
       "\n",
       "   chunk_size       embedding_function  \\\n",
       "0           5  OpenAIEmbeddingFunction   \n",
       "1          10  OpenAIEmbeddingFunction   \n",
       "2          15  OpenAIEmbeddingFunction   \n",
       "3          20  OpenAIEmbeddingFunction   \n",
       "\n",
       "                                       config  \n",
       "0   SentenceChunker_5_OpenAIEmbeddingFunction  \n",
       "1  SentenceChunker_10_OpenAIEmbeddingFunction  \n",
       "2  SentenceChunker_15_OpenAIEmbeddingFunction  \n",
       "3  SentenceChunker_20_OpenAIEmbeddingFunction  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining our Configurations\n",
    "chunkers = [\n",
    "    SentenceChunker(sentences_per_chunk = 5),\n",
    "    SentenceChunker(sentences_per_chunk = 10),\n",
    "    SentenceChunker(sentences_per_chunk = 15),\n",
    "    SentenceChunker(sentences_per_chunk = 20),\n",
    "]\n",
    "\n",
    "# Defining our Embedding Functions\n",
    "embedders = [\n",
    "    embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_base=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model_name=\"text-embedding-nomic-embed-text-v1.5-embedding\"\n",
    ")\n",
    "\n",
    "]\n",
    "\n",
    "# Initialize Results Storage\n",
    "synth_results = []\n",
    "\n",
    "# Helper Function\n",
    "def get_config_name(chunker, ef):\n",
    "    chunk_size = chunker.sentences_per_chunk if hasattr(chunker, 'sentences_per_chunk') else 0\n",
    "    ef_name = ef.model_name if hasattr(ef, 'model_name') else ef.__class__.__name__\n",
    "    return f\"{chunker.__class__.__name__}_{chunk_size}_{ef_name}\"\n",
    "\n",
    "# Progress tracking\n",
    "total_combinations = len(chunkers) * len(embedders)\n",
    "current_combination = 0\n",
    "\n",
    "# Run evaluation sweep\n",
    "for chunker in chunkers:\n",
    "    for ef in embedders:\n",
    "        current_combination += 1\n",
    "        try:\n",
    "            print(f\"Evaluating combination {current_combination}/{total_combinations}:\")\n",
    "            print(f\"  Chunker: {chunker.__class__.__name__} (size: {chunker.sentences_per_chunk})\")\n",
    "            print(f\"  Embedding: {ef.model_name if hasattr(ef, 'model_name') else ef.__class__.__name__}\")\n",
    "            \n",
    "            # Run evaluation\n",
    "            result = synthetic_pipeline.run(chunker, ef, retrieve=5)\n",
    "            \n",
    "            # Clean up and store results\n",
    "            if 'corpora_scores' in result:\n",
    "                del result['corpora_scores']\n",
    "            \n",
    "            # Add configuration identifiers\n",
    "            result['chunker'] = chunker.__class__.__name__\n",
    "            result['chunk_size'] = chunker.sentences_per_chunk\n",
    "            result['embedding_function'] = ef.model_name if hasattr(ef, 'model_name') else ef.__class__.__name__\n",
    "            result['config'] = get_config_name(chunker, ef)\n",
    "            \n",
    "            synth_results.append(result)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Error Handling Just in Case\n",
    "            print(f\"Error in combination {current_combination}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Create final DataFrame and display\n",
    "synth_df = pd.DataFrame(synth_results)\n",
    "print(\"\\nFinal Results:\")\n",
    "display(synth_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b2c788c-7e4a-425c-b3de-1b89dcd41c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_df.to_excel(\"metrics_results.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49266a0-2872-4493-b666-84800b0544d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025",
   "language": "python",
   "name": "papers_2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
