{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5c6bac-b9f5-4ccd-bc59-c8f2a6a6b5e8",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024c5794-191d-4192-ab34-af12af951a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryanjadon/miniconda3/envs/papers_2025/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import pdb\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from importlib import resources\n",
    "from IPython.display import display, clear_output\n",
    "from chunking_evaluation.utils import rigorous_document_search\n",
    "from chunking_evaluation.evaluation_framework.base_evaluation import BaseEvaluation\n",
    "\n",
    "class SyntheticEvaluation(BaseEvaluation):\n",
    "    def __init__(self, corpora_paths: List[str], queries_csv_path: str, chroma_db_path: str = None,\n",
    "                 openai_api_key=None):\n",
    "        super().__init__(questions_csv_path=queries_csv_path, chroma_db_path=chroma_db_path)\n",
    "        self.corpora_paths = corpora_paths\n",
    "        self.questions_csv_path = queries_csv_path\n",
    "        self.client = OpenAI(base_url=\"Your-URL-HERE\", api_key=\"XXXX-Your-API-Key-Here-XXX\")\n",
    "        self.synth_questions_df = None\n",
    "\n",
    "        with resources.as_file(resources.files('chunking_evaluation.evaluation_framework') / 'prompts') as prompt_path:\n",
    "            with open(os.path.join(prompt_path, 'question_maker_system.txt'), 'r') as f:\n",
    "                self.question_maker_system_prompt = f.read()\n",
    "\n",
    "            with open(os.path.join(prompt_path, 'question_maker_approx_system.txt'), 'r') as f:\n",
    "                self.question_maker_approx_system_prompt = f.read()\n",
    "\n",
    "            with open(os.path.join(prompt_path, 'question_maker_user.txt'), 'r') as f:\n",
    "                self.question_maker_user_prompt = f.read()\n",
    "\n",
    "            with open(os.path.join(prompt_path, 'question_maker_approx_user.txt'), 'r') as f:\n",
    "                self.question_maker_approx_user_prompt = f.read()\n",
    "\n",
    "    def _save_questions_df(self):\n",
    "        self.synth_questions_df.to_csv(self.questions_csv_path, index=False)\n",
    "\n",
    "    def _tag_text(self, text):\n",
    "        chunk_length = 100\n",
    "        chunks = []\n",
    "        tag_indexes = [0]\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_length\n",
    "            chunk = text[start:end]\n",
    "            if end < len(text):\n",
    "                # Find the last space within the chunk to avoid splitting a word\n",
    "                space_index = chunk.rfind(' ')\n",
    "                if space_index != -1:\n",
    "                    end = start + space_index + 1  # Include the space in the chunk\n",
    "                    chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            tag_indexes.append(end)\n",
    "            start = end  # Move start to end to continue splitting\n",
    "\n",
    "        tagged_text = \"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            tagged_text += f\"<start_chunk_{i}>\" + chunk + f\"<end_chunk_{i}>\"\n",
    "\n",
    "        return tagged_text, tag_indexes\n",
    "\n",
    "    def convert_text_to_json(self,text: str):\n",
    "        \"\"\"\n",
    "        Removes the text between <think> and </think> tags and converts\n",
    "        the remaining text (assumed to be a Python dictionary literal)\n",
    "        into a JSON string.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The original text containing a <think>...</think> block\n",
    "                        and the dictionary to convert.\n",
    "    \n",
    "        Returns:\n",
    "            str: A JSON-formatted string representing the dictionary.\n",
    "        \"\"\"\n",
    "        # Remove everything between <think> and </think> (including the tags)\n",
    "        cleaned_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "        # Strip any leading/trailing whitespace\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "    \n",
    "        # Parse the remaining text as a Python literal.\n",
    "        # (Using ast.literal_eval since the text is written as a Python dictionary.)\n",
    "        try:\n",
    "            data = ast.literal_eval(cleaned_text)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Failed to parse the remaining text as a dictionary: \" + str(e))\n",
    "    \n",
    "        # Convert the Python dictionary to a JSON-formatted string\n",
    "        json_response = json.dumps(data, indent=2)\n",
    "        \n",
    "        return json_response\n",
    "\n",
    "    def _extract_question_and_approx_references(self, corpus, document_length=4000, prev_questions=[]):\n",
    "        \n",
    "        if len(corpus) > document_length:\n",
    "            start_index = random.randint(0, len(corpus) - document_length)\n",
    "            document = corpus[start_index: start_index + document_length]\n",
    "        else:\n",
    "            start_index = 0\n",
    "            document = corpus\n",
    "\n",
    "        if prev_questions is not None:\n",
    "            if len(prev_questions) > 20:\n",
    "                questions_sample = random.sample(prev_questions, 20)\n",
    "                prev_questions_str = '\\n'.join(questions_sample)\n",
    "            else:\n",
    "                prev_questions_str = '\\n'.join(prev_questions)\n",
    "        else:\n",
    "            prev_questions_str = \"\"\n",
    "\n",
    "        tagged_text, tag_indexes = self._tag_text(document)\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.question_maker_approx_system_prompt},\n",
    "                {\"role\": \"user\",\n",
    "                 \"content\": self.question_maker_approx_user_prompt.replace(\"{document}\", tagged_text).replace(\n",
    "                     \"{prev_questions_str}\", prev_questions_str)}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response = completion.choices[0].message.content\n",
    "        json_response = json.loads(self.convert_text_to_json(response))\n",
    "        \n",
    "        print(json_response)\n",
    "        \n",
    "        try:\n",
    "            text_references = json_response['references']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"The response does not contain a 'references' field.\")\n",
    "\n",
    "        try:\n",
    "            question = json_response['question']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"The response does not contain a 'question' field.\")\n",
    "\n",
    "        \n",
    "        references = []\n",
    "    \n",
    "        for reference in text_references:\n",
    "            try:\n",
    "                reference_keys = list(reference.keys())\n",
    "    \n",
    "                if len(reference_keys) != 3:\n",
    "                    raise ValueError(\n",
    "                        f\"Each reference must have exactly 3 keys: 'content', 'start_chunk', and 'end_chunk'. Got keys: {reference_keys}\")\n",
    "    \n",
    "                if 'start_chunk' not in reference_keys or 'end_chunk' not in reference_keys:\n",
    "                    raise ValueError(\"Each reference must contain 'start_chunk' and 'end_chunk' keys.\")\n",
    "    \n",
    "                if 'end_chunk' not in reference_keys:\n",
    "                    reference_keys.remove('content')\n",
    "                    reference_keys.remove('start_chunk')\n",
    "                    end_chunk_key = reference_keys[0]\n",
    "                    end_index = start_index + tag_indexes[reference[end_chunk_key] + 1]\n",
    "                else:\n",
    "                    end_index = start_index + tag_indexes[reference['end_chunk'] + 1]\n",
    "    \n",
    "                start_index = start_index + tag_indexes[reference['start_chunk']]\n",
    "                references.append((corpus[start_index:end_index], start_index, end_index))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "        return question, references\n",
    "\n",
    "    def _extract_question_and_references(self, corpus, document_length=4000, prev_questions=[]):\n",
    "        if len(corpus) > document_length:\n",
    "            start_index = random.randint(0, len(corpus) - document_length)\n",
    "            document = corpus[start_index: start_index + document_length]\n",
    "        else:\n",
    "            document = corpus\n",
    "\n",
    "        if prev_questions is not None:\n",
    "            if len(prev_questions) > 20:\n",
    "                questions_sample = random.sample(prev_questions, 20)\n",
    "                prev_questions_str = '\\n'.join(questions_sample)\n",
    "            else:\n",
    "                prev_questions_str = '\\n'.join(prev_questions)\n",
    "        else:\n",
    "            prev_questions_str = \"\"\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.question_maker_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": self.question_maker_user_prompt.replace(\"{document}\", document).replace(\n",
    "                    \"{prev_questions_str}\", prev_questions_str)}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response = completion.choices[0].message\n",
    "        json_response = json.loads(response.json())\n",
    "        \n",
    "        input_str = json_response['content']\n",
    "        new_string = input_str.replace(\"```json\", \"```\")\n",
    "        cleaned_string = new_string.strip().strip('```').strip()\n",
    "            \n",
    "        parsed_dict = json.loads(input_str)\n",
    "        print(parsed_dict)\n",
    "        \n",
    "        json_response = parsed_dict\n",
    "\n",
    "\n",
    "        try:\n",
    "            text_references = json_response['references']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"The response does not contain a 'references' field.\")\n",
    "\n",
    "        try:\n",
    "            question = json_response['question']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"The response does not contain a 'question' field.\")\n",
    "\n",
    "        references = []\n",
    "        for reference in text_references:\n",
    "            if not isinstance(reference, str):\n",
    "                raise ValueError(f\"Expected reference to be of type str, but got {type(reference).__name__}\")\n",
    "            target = rigorous_document_search(corpus, reference)\n",
    "            if target is not None:\n",
    "                reference, start_index, end_index = target\n",
    "                references.append((reference, start_index, end_index))\n",
    "            else:\n",
    "                raise ValueError(f\"No match found in the document for the given reference.\\nReference: {reference}\")\n",
    "\n",
    "        return question, references\n",
    "\n",
    "    def _generate_corpus_questions(self, corpus_id, approx=False, n=5):\n",
    "        with open(corpus_id, 'r') as file:\n",
    "            corpus = file.read()\n",
    "\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            while True:\n",
    "                try:\n",
    "                    print(f\"Trying Query {i}\")\n",
    "                    questions_list = self.synth_questions_df[self.synth_questions_df['corpus_id'] == corpus_id][\n",
    "                        'question'].tolist()\n",
    "                    if approx:\n",
    "                        question, references = self._extract_question_and_approx_references(corpus, 4000,\n",
    "                                                                                            questions_list)\n",
    "                    else:\n",
    "                        question, references = self._extract_question_and_references(corpus, 4000, questions_list)\n",
    "                    if len(references) > 5:\n",
    "                        raise ValueError(\"The number of references exceeds 5.\")\n",
    "\n",
    "                    references = [{'content': ref[0], 'start_index': ref[1], 'end_index': ref[2]} for ref in references]\n",
    "                    new_question = {\n",
    "                        'question': question,\n",
    "                        'references': json.dumps(references),\n",
    "                        'corpus_id': corpus_id\n",
    "                    }\n",
    "\n",
    "                    new_df = pd.DataFrame([new_question])\n",
    "                    self.synth_questions_df = pd.concat([self.synth_questions_df, new_df], ignore_index=True)\n",
    "                    self._save_questions_df()\n",
    "\n",
    "                    break\n",
    "                except (ValueError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error occurred: {e}\")\n",
    "                    continue\n",
    "            i += 1\n",
    "\n",
    "    def _get_synth_questions_df(self):\n",
    "        if os.path.exists(self.questions_csv_path):\n",
    "            synth_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "        else:\n",
    "            synth_questions_df = pd.DataFrame(columns=['question', 'references', 'corpus_id'])\n",
    "        return synth_questions_df\n",
    "\n",
    "    def generate_queries_and_excerpts(self, approximate_excerpts=False, num_rounds=-1, queries_per_corpus=5):\n",
    "        self.synth_questions_df = self._get_synth_questions_df()\n",
    "\n",
    "        rounds = 0\n",
    "        while num_rounds == -1 or rounds < num_rounds:\n",
    "            for corpus_id in self.corpora_paths:\n",
    "                self._generate_corpus_questions(corpus_id, approx=approximate_excerpts, n=queries_per_corpus)\n",
    "            rounds += 1\n",
    "\n",
    "    def _get_sim(self, target, references):\n",
    "        response = self.client.embeddings.create(\n",
    "            input=[target] + references,\n",
    "            model=\"text-embedding-bge-m3\"\n",
    "        )\n",
    "        nparray1 = np.array(response.data[0].embedding)\n",
    "\n",
    "        full_sim = []\n",
    "        for i in range(1, len(response.data)):\n",
    "            nparray2 = np.array(response.data[i].embedding)\n",
    "            cosine_similarity = np.dot(nparray1, nparray2) / (np.linalg.norm(nparray1) * np.linalg.norm(nparray2))\n",
    "            full_sim.append(cosine_similarity)\n",
    "\n",
    "        return full_sim\n",
    "\n",
    "    def _corpus_filter_poor_highlights(self, corpus_id, synth_questions_df, threshold):\n",
    "        corpus_questions_df = synth_questions_df[synth_questions_df['corpus_id'] == corpus_id]\n",
    "\n",
    "        def edit_row(row):\n",
    "            question = row['question']\n",
    "            references = [ref['content'] for ref in row['references']]\n",
    "            similarity_scores = self._get_sim(question, references)\n",
    "            worst_ref_score = min(similarity_scores)\n",
    "            row['worst_ref_score'] = worst_ref_score\n",
    "            return row\n",
    "\n",
    "        # Apply the function to each row\n",
    "        corpus_questions_df = corpus_questions_df.apply(edit_row, axis=1)\n",
    "\n",
    "        count_before = len(corpus_questions_df)\n",
    "\n",
    "        corpus_questions_df = corpus_questions_df[corpus_questions_df['worst_ref_score'] >= threshold]\n",
    "        corpus_questions_df = corpus_questions_df.drop(columns=['worst_ref_score'])\n",
    "\n",
    "        count_after = len(corpus_questions_df)\n",
    "\n",
    "        print(f\"Corpus: {corpus_id} - Removed {count_before - count_after} .\")\n",
    "\n",
    "        corpus_questions_df['references'] = corpus_questions_df['references'].apply(json.dumps)\n",
    "\n",
    "        full_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "        full_questions_df = full_questions_df[full_questions_df['corpus_id'] != corpus_id]\n",
    "\n",
    "        full_questions_df = pd.concat([full_questions_df, corpus_questions_df], ignore_index=True)\n",
    "        # Drop the columns 'fixed', 'worst_ref_score' and 'diff_score' if they exist\n",
    "        for col in ['fixed', 'worst_ref_score', 'diff_score']:\n",
    "            if col in full_questions_df.columns:\n",
    "                full_questions_df = full_questions_df.drop(columns=col)\n",
    "\n",
    "        full_questions_df.to_csv(self.questions_csv_path, index=False)\n",
    "\n",
    "    def filter_poor_excerpts(self, threshold=0.36, corpora_subset=[]):\n",
    "        if os.path.exists(self.questions_csv_path):\n",
    "            synth_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "            if len(synth_questions_df) > 0:\n",
    "                synth_questions_df['references'] = synth_questions_df['references'].apply(json.loads)\n",
    "                corpus_list = synth_questions_df['corpus_id'].unique().tolist()\n",
    "                if corpora_subset:\n",
    "                    corpus_list = [c for c in corpus_list if c in corpora_subset]\n",
    "                for corpus_id in corpus_list:\n",
    "                    self._corpus_filter_poor_highlights(corpus_id, synth_questions_df, threshold)\n",
    "\n",
    "    def _corpus_filter_duplicates(self, corpus_id, synth_questions_df, threshold):\n",
    "        corpus_questions_df = synth_questions_df[synth_questions_df['corpus_id'] == corpus_id].copy()\n",
    "\n",
    "        count_before = len(corpus_questions_df)\n",
    "\n",
    "        corpus_questions_df.drop_duplicates(subset='question', keep='first', inplace=True)\n",
    "\n",
    "        questions = corpus_questions_df['question'].tolist()\n",
    "\n",
    "        response = self.client.embeddings.create(\n",
    "            input=questions,\n",
    "            model=\"text-embedding-bge-m3\"\n",
    "        )\n",
    "\n",
    "        embeddings_matrix = np.array([data.embedding for data in response.data])\n",
    "\n",
    "        dot_product_matrix = np.dot(embeddings_matrix, embeddings_matrix.T)\n",
    "\n",
    "        # Create a list of tuples containing the index pairs and their similarity\n",
    "        similarity_pairs = [(i, j, dot_product_matrix[i][j]) for i in range(len(dot_product_matrix)) for j in\n",
    "                            range(i + 1, len(dot_product_matrix))]\n",
    "\n",
    "        # Sort the list of tuples based on the similarity in descending order\n",
    "        similarity_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        similarity_scores = np.array([x[2] for x in similarity_pairs])\n",
    "\n",
    "        most_similars = (dot_product_matrix - np.eye(dot_product_matrix.shape[0])).max(axis=1)\n",
    "\n",
    "        def filter_vectors(sim_matrix, threshold):\n",
    "            n = sim_matrix.shape[0]  # Number of vectors\n",
    "            remaining = np.ones(n, dtype=bool)  # Initialize all vectors as remaining\n",
    "\n",
    "            for i in range(n):\n",
    "                if remaining[i] == 1:  # Only check for vectors that are still remaining\n",
    "                    for j in range(i + 1, n):\n",
    "                        if remaining[j] == 1 and sim_matrix[i, j] > threshold:\n",
    "                            remaining[j] = 0  # Remove vector j because it's too similar to vector i\n",
    "\n",
    "            return remaining\n",
    "\n",
    "        rows_to_keep = filter_vectors(dot_product_matrix, threshold)\n",
    "\n",
    "        corpus_questions_df = corpus_questions_df[rows_to_keep]\n",
    "\n",
    "        count_after = len(corpus_questions_df)\n",
    "\n",
    "        print(f\"Corpus: {corpus_id} - Removed {count_before - count_after} .\")\n",
    "\n",
    "        corpus_questions_df['references'] = corpus_questions_df['references'].apply(json.dumps)\n",
    "\n",
    "        full_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "        full_questions_df = full_questions_df[full_questions_df['corpus_id'] != corpus_id]\n",
    "\n",
    "        full_questions_df = pd.concat([full_questions_df, corpus_questions_df], ignore_index=True)\n",
    "        # Drop the columns 'fixed', 'worst_ref_score' and 'diff_score' if they exist\n",
    "        for col in ['fixed', 'worst_ref_score', 'diff_score']:\n",
    "            if col in full_questions_df.columns:\n",
    "                full_questions_df = full_questions_df.drop(columns=col)\n",
    "\n",
    "        full_questions_df.to_csv(self.questions_csv_path, index=False)\n",
    "\n",
    "    def filter_duplicates(self, threshold=0.78, corpora_subset=[]):\n",
    "        if os.path.exists(self.questions_csv_path):\n",
    "            synth_questions_df = pd.read_csv(self.questions_csv_path)\n",
    "            if len(synth_questions_df) > 0:\n",
    "                synth_questions_df['references'] = synth_questions_df['references'].apply(json.loads)\n",
    "                corpus_list = synth_questions_df['corpus_id'].unique().tolist()\n",
    "                if corpora_subset:\n",
    "                    corpus_list = [c for c in corpus_list if c in corpora_subset]\n",
    "                for corpus_id in corpus_list:\n",
    "                    self._corpus_filter_duplicates(corpus_id, synth_questions_df, threshold)\n",
    "\n",
    "    def question_ref_filter(self):\n",
    "        self.synth_questions_df = self._get_synth_questions_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec4592-c4f4-4589-a9c5-585db0d23477",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d367bf-02ce-420b-a1e9-428c72fa9440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying Query 0\n"
     ]
    }
   ],
   "source": [
    "# Specify the corpora paths, you can have multiple but we'll just use our one file\n",
    "corpora_paths = [\n",
    "    'chunking_evaluation/finance_data/amazon.txt',\n",
    "    'chunking_evaluation/finance_data/apple.txt',\n",
    "    'chunking_evaluation/finance_data/broadcom.txt',\n",
    "    'chunking_evaluation/finance_data/google.txt',\n",
    "    'chunking_evaluation/finance_data/meta.txt',\n",
    "    'chunking_evaluation/finance_data/microsoft.txt',\n",
    "    'chunking_evaluation/finance_data/netflix.txt',\n",
    "    'chunking_evaluation/finance_data/nvidia.txt',\n",
    "    'chunking_evaluation/finance_data/tesla.txt',\n",
    "    'chunking_evaluation/finance_data/tsmc.txt',\n",
    "]\n",
    "\n",
    "csv_path = 'chunking_evaluation/finance_results_DeepSeek-R1-Distill-Qwen-32B.csv'\n",
    "\n",
    "# Initialize the evaluation\n",
    "synthetic_pipeline = SyntheticEvaluation(corpora_paths, csv_path)\n",
    "synthetic_pipeline.generate_queries_and_excerpts(approximate_excerpts=True,\n",
    "                                         num_rounds=1,\n",
    "                                         queries_per_corpus=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193cea92-a78f-4d3b-8ac8-0998811f9b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025",
   "language": "python",
   "name": "papers_2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
